{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avimallick/Deep-Learning-Assignments/blob/master/19113015_DL_Exercise9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DEEP LEARNING ASSIGNMENT 4\n",
        "\n",
        "## AVINASH MALLICK\n",
        "## 19113015\n",
        "## CSE-7A\n",
        "\n",
        "# Differentiable Neural Computer"
      ],
      "metadata": {
        "id": "9rIsvBqSUS7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "import os"
      ],
      "metadata": {
        "id": "iC1EPsg9UTOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DNC:\n",
        "    def __init__(self, input_size, output_size, seq_len, num_words=256, word_size=64, num_heads=4):\n",
        "        #define data\n",
        "        #input data - [[1 0] [0 1] [0 0] [0 0]]\n",
        "        self.input_size = input_size #X\n",
        "        #output data [[0 0] [0 0] [1 0] [0 1]]\n",
        "        self.output_size = output_size #Y\n",
        "        \n",
        "        #define read + write vector size\n",
        "        #10\n",
        "        self.num_words = num_words #N\n",
        "        #4 characters\n",
        "        self.word_size = word_size #W\n",
        "        \n",
        "        #define number of read+write heads\n",
        "        #we could have multiple, but just 1 for simplicity\n",
        "        self.num_heads = num_heads #R\n",
        "\n",
        "        #size of output vector from controller that defines interactions with memory matrix\n",
        "        self.interface_size = num_heads*word_size + 3*word_size + 5*num_heads + 3\n",
        "\n",
        "        #the actual size of the neural network input after flatenning and\n",
        "        # concatenating the input vector with the previously read vctors from memory\n",
        "        self.nn_input_size = num_heads * word_size + input_size\n",
        "        \n",
        "        #size of output\n",
        "        self.nn_output_size = output_size + self.interface_size\n",
        "        \n",
        "        #gaussian normal distribution for both outputs\n",
        "        self.nn_out = tf.truncated_normal([1, self.output_size], stddev=0.1)\n",
        "        self.interface_vec = tf.truncated_normal([1, self.interface_size], stddev=0.1)\n",
        "\n",
        "        #Create memory matrix\n",
        "        self.mem_mat = tf.zeros([num_words, word_size]) #N*W\n",
        "        \n",
        "        #other variables\n",
        "        #The usage vector records which locations have been used so far, \n",
        "        self.usage_vec = tf.fill([num_words, 1], 1e-6) #N*1\n",
        "        #a temporal link matrix records the order in which locations were written;\n",
        "        self.link_mat = tf.zeros([num_words,num_words]) #N*N\n",
        "        #represents degrees to which last location was written to\n",
        "        self.precedence_weight = tf.zeros([num_words, 1]) #N*1\n",
        "\n",
        "        #Read and write head variables\n",
        "        self.read_weights = tf.fill([num_words, num_heads], 1e-6) #N*R\n",
        "        self.write_weights = tf.fill([num_words, 1], 1e-6) #N*1\n",
        "        self.read_vecs = tf.fill([num_heads, word_size], 1e-6) #R*W\n",
        "\n",
        "        ###NETWORK VARIABLES\n",
        "        #gateways into the computation graph for input output pairs\n",
        "        self.i_data = tf.placeholder(tf.float32, [seq_len*2, self.input_size], name='input_node')\n",
        "        self.o_data = tf.placeholder(tf.float32, [seq_len*2, self.output_size], name='output_node')\n",
        "        \n",
        "        #2 layer feedforwarded network\n",
        "        self.W1 = tf.Variable(tf.truncated_normal([self.nn_input_size, 32], stddev=0.1), name='layer1_weights', dtype=tf.float32)\n",
        "        self.b1 = tf.Variable(tf.zeros([32]), name='layer1_bias', dtype=tf.float32)\n",
        "        self.W2 = tf.Variable(tf.truncated_normal([32, self.nn_output_size], stddev=0.1), name='layer2_weights', dtype=tf.float32)\n",
        "        self.b2 = tf.Variable(tf.zeros([self.nn_output_size]), name='layer2_bias', dtype=tf.float32)\n",
        "\n",
        "        ###DNC OUTPUT WEIGHTS\n",
        "        self.nn_out_weights = tf.Variable(tf.truncated_normal([self.nn_output_size, self.output_size], stddev=0.1), name='net_output_weights')\n",
        "        self.interface_weights = tf.Variable(tf.truncated_normal([self.nn_output_size, self.interface_size], stddev=0.1), name='interface_weights')\n",
        "        \n",
        "        self.read_vecs_out_weight = tf.Variable(tf.truncated_normal([self.num_heads*self.word_size, self.output_size], stddev=0.1), name='read_vector_weights')\n",
        "\n",
        "    #3 attention mechanisms for read/writes to memory \n",
        "    \n",
        "    #1\n",
        "    #a key vector emitted by the controller is compared to the \n",
        "    #content of each location in memory according to a similarity measure \n",
        "    #The similarity scores determine a weighting that can be used by the read heads \n",
        "    #for associative recall1 or by the write head to modify an existing vector in memory.\n",
        "    def content_lookup(self, key, str):\n",
        "        #The l2 norm of a vector is the square root of the sum of the \n",
        "        #absolute values squared\n",
        "        norm_mem = tf.nn.l2_normalize(self.mem_mat, 1) #N*W\n",
        "        norm_key = tf.nn.l2_normalize(key, 0) #1*W for write or R*W for read\n",
        "        #get similarity measure between both vectors, transpose before multiplicaiton\n",
        "        ##(N*W,W*1)->N*1 for write\n",
        "        #(N*W,W*R)->N*R for read\n",
        "        sim = tf.matmul(norm_mem, norm_key, transpose_b=True) \n",
        "        #str is 1*1 or 1*R\n",
        "        #returns similarity measure\n",
        "        return tf.nn.softmax(sim*str, 0) #N*1 or N*R\n",
        "\n",
        "    #2\n",
        "    #retreives the writing allocation weighting based on the usage free list\n",
        "    #The ‘usage’ of each location is represented as a number between 0 and 1, \n",
        "    #and a weighting that picks out unused locations is delivered to the write head. \n",
        "    \n",
        "    # independent of the size and contents of the memory, meaning that \n",
        "    #DNCs can be trained to solve a task using one size of memory and later \n",
        "    #upgraded to a larger memory without retraining\n",
        "    def allocation_weighting(self):\n",
        "        #sorted usage - the usage vector sorted ascndingly\n",
        "        #the original indices of the sorted usage vector\n",
        "        sorted_usage_vec, free_list = tf.nn.top_k(-1 * self.usage_vec, k=self.num_words)\n",
        "        sorted_usage_vec *= -1\n",
        "        cumprod = tf.cumprod(sorted_usage_vec, axis=0, exclusive=True)\n",
        "        unorder = (1-sorted_usage_vec)*cumprod\n",
        "\n",
        "        alloc_weights = tf.zeros([self.num_words])\n",
        "        I = tf.constant(np.identity(self.num_words, dtype=np.float32))\n",
        "        \n",
        "        #for each usage vec\n",
        "        for pos, idx in enumerate(tf.unstack(free_list[0])):\n",
        "            #flatten\n",
        "            m = tf.squeeze(tf.slice(I, [idx, 0], [1, -1]))\n",
        "            #add to weight matrix\n",
        "            alloc_weights += m*unorder[0, pos]\n",
        "        #the allocation weighting for each row in memory\n",
        "        return tf.reshape(alloc_weights, [self.num_words, 1])\n",
        "\n",
        "    #at every time step the controller receives input vector from dataset and emits output vector. \n",
        "    #it also recieves a set of read vectors from the memory matrix at the previous time step via \n",
        "    #the read heads. then it emits an interface vector that defines its interactions with the memory\n",
        "    #at the current time step\n",
        "    def step_m(self, x):\n",
        "        \n",
        "        #reshape input\n",
        "        input = tf.concat([x, tf.reshape(self.read_vecs, [1, self.num_heads*self.word_size])],1)\n",
        "        \n",
        "        #forward propagation\n",
        "        l1_out = tf.matmul(input, self.W1) + self.b1\n",
        "        l1_act = tf.nn.tanh(l1_out)\n",
        "        l2_out = tf.matmul(l1_act, self.W2) + self.b2\n",
        "        l2_act = tf.nn.tanh(l2_out)\n",
        "        \n",
        "        #output vector\n",
        "        self.nn_out = tf.matmul(l2_act, self.nn_out_weights) #(1*eta+Y, eta+Y*Y)->(1*Y)\n",
        "        #interaction vector - how to interact with memory\n",
        "        self.interface_vec = tf.matmul(l2_act, self.interface_weights) #(1*eta+Y, eta+Y*eta)->(1*eta)\n",
        "        \n",
        "        \n",
        "        partition = tf.constant([[0]*(self.num_heads*self.word_size) + [1]*(self.num_heads) + [2]*(self.word_size) + [3] + \\\n",
        "                    [4]*(self.word_size) + [5]*(self.word_size) + \\\n",
        "                    [6]*(self.num_heads) + [7] + [8] + [9]*(self.num_heads*3)], dtype=tf.int32)\n",
        "\n",
        "        #convert interface vector into a set of read write vectors\n",
        "        #using tf.dynamic_partitions(Partitions interface_vec into 10 tensors using indices from partition)\n",
        "        (read_keys, read_str, write_key, write_str,\n",
        "         erase_vec, write_vec, free_gates, alloc_gate, write_gate, read_modes) = \\\n",
        "            tf.dynamic_partition(self.interface_vec, partition, 10)\n",
        "        \n",
        "        #read vectors\n",
        "        read_keys = tf.reshape(read_keys,[self.num_heads, self.word_size]) #R*W\n",
        "        read_str = 1 + tf.nn.softplus(tf.expand_dims(read_str, 0)) #1*R\n",
        "        \n",
        "        #write vectors\n",
        "        write_key = tf.expand_dims(write_key, 0) #1*W\n",
        "        #help init our write weights\n",
        "        write_str = 1 + tf.nn.softplus(tf.expand_dims(write_str, 0)) #1*1\n",
        "        erase_vec = tf.nn.sigmoid(tf.expand_dims(erase_vec, 0)) #1*W\n",
        "        write_vec = tf.expand_dims(write_vec, 0) #1*W\n",
        "        \n",
        "        #the degree to which locations at read heads will be freed\n",
        "        free_gates = tf.nn.sigmoid(tf.expand_dims(free_gates, 0)) #1*R\n",
        "        #the fraction of writing that is being allocated in a new location\n",
        "        alloc_gate = tf.nn.sigmoid(alloc_gate) #1\n",
        "        #the amount of information to be written to memory\n",
        "        write_gate = tf.nn.sigmoid(write_gate) #1\n",
        "        #the softmax distribution between the three read modes (backward, forward, lookup)\n",
        "        #The read heads can use gates called read modes to switch between content lookup \n",
        "        #using a read key and reading out locations either forwards or backwards \n",
        "        #in the order they were written.\n",
        "        read_modes = tf.nn.softmax(tf.reshape(read_modes, [3, self.num_heads])) #3*R\n",
        "        \n",
        "        #used to calculate usage vector, what's available to write to?\n",
        "        retention_vec = tf.reduce_prod(1-free_gates*self.read_weights, reduction_indices=1)\n",
        "        #used to dynamically allocate memory\n",
        "        self.usage_vec = (self.usage_vec + self.write_weights - self.usage_vec * self.write_weights) * retention_vec\n",
        "\n",
        "        ##retreives the writing allocation weighting \n",
        "        alloc_weights = self.allocation_weighting() #N*1\n",
        "        #where to write to??\n",
        "        write_lookup_weights = self.content_lookup(write_key, write_str) #N*1\n",
        "        #define our write weights now that we know how much space to allocate for them and where to write to\n",
        "        self.write_weights = write_gate*(alloc_gate*alloc_weights + (1-alloc_gate)*write_lookup_weights)\n",
        "\n",
        "        #write erase, then write to memory!\n",
        "        self.mem_mat = self.mem_mat*(1-tf.matmul(self.write_weights, erase_vec)) + \\\n",
        "                       tf.matmul(self.write_weights, write_vec)\n",
        "\n",
        "        #As well as writing, the controller can read from multiple locations in memory. \n",
        "        #Memory can be searched based on the content of each location, or the associative \n",
        "        #temporal links can be followed forward and backward to recall information written \n",
        "        #in sequence or in reverse. (3rd attention mechanism)\n",
        "        \n",
        "        #updates and returns the temporal link matrix for the latest write\n",
        "        #given the precedence vector and the link matrix from previous step\n",
        "        nnweight_vec = tf.matmul(self.write_weights, tf.ones([1,self.num_words])) #N*N\n",
        "        self.link_mat = (1 - nnweight_vec - tf.transpose(nnweight_vec))*self.link_mat + \\\n",
        "                        tf.matmul(self.write_weights, self.precedence_weight, transpose_b=True)\n",
        "        self.link_mat *= tf.ones([self.num_words, self.num_words]) - tf.constant(np.identity(self.num_words, dtype=np.float32))\n",
        "\n",
        "        \n",
        "        self.precedence_weight = (1-tf.reduce_sum(self.write_weights, reduction_indices=0)) * \\\n",
        "                                 self.precedence_weight + self.write_weights\n",
        "        #3 modes - forward, backward, content lookup\n",
        "        forw_w = read_modes[2]*tf.matmul(self.link_mat, self.read_weights) #(N*N,N*R)->N*R\n",
        "        look_w = read_modes[1]*self.content_lookup(read_keys, read_str) #N*R\n",
        "        back_w = read_modes[0]*tf.matmul(self.link_mat, self.read_weights, transpose_a=True) #N*R\n",
        "\n",
        "        #use them to intiialize read weights\n",
        "        self.read_weights = back_w + look_w + forw_w #N*R\n",
        "        #create read vectors by applying read weights to memory matrix\n",
        "        self.read_vecs = tf.transpose(tf.matmul(self.mem_mat, self.read_weights, transpose_a=True)) #(W*N,N*R)^T->R*W\n",
        "\n",
        "        #multiply them together\n",
        "        read_vec_mut = tf.matmul(tf.reshape(self.read_vecs, [1, self.num_heads * self.word_size]),\n",
        "                                 self.read_vecs_out_weight)  # (1*RW, RW*Y)-> (1*Y)\n",
        "        \n",
        "        #return output + read vecs product\n",
        "        return self.nn_out+read_vec_mut\n",
        "\n",
        "    #output list of numbers (one hot encoded) by running the step function\n",
        "    def run(self):\n",
        "        big_out = []\n",
        "        for t, seq in enumerate(tf.unstack(self.i_data, axis=0)):\n",
        "            seq = tf.expand_dims(seq, 0)\n",
        "            y = self.step_m(seq)\n",
        "            big_out.append(y)\n",
        "        return tf.stack(big_out, axis=0)"
      ],
      "metadata": {
        "id": "S9aUr-9mUaym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(argv=None):\n",
        "\n",
        "    #generate the input output sequences, randomly intialized\n",
        "    num_seq = 10\n",
        "    seq_len = 6\n",
        "    seq_width = 4\n",
        "    iterations = 1000\n",
        "    con = np.random.randint(0, seq_width,size=seq_len)\n",
        "    seq = np.zeros((seq_len, seq_width))\n",
        "    seq[np.arange(seq_len), con] = 1\n",
        "    end = np.asarray([[-1]*seq_width])\n",
        "    zer = np.zeros((seq_len, seq_width))\n",
        "\n",
        "    graph = tf.Graph()\n",
        "    \n",
        "    with graph.as_default():\n",
        "        #training time\n",
        "        with tf.Session() as sess:\n",
        "            #init the DNC\n",
        "            dnc = DNC(input_size=seq_width, output_size=seq_width, seq_len=seq_len, num_words=10, word_size=4, num_heads=1)\n",
        "            \n",
        "            #calculate the predicted output\n",
        "            output = tf.squeeze(dnc.run())\n",
        "            #compare prediction to reality, get loss via sigmoid cross entropy\n",
        "            loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=output, labels=dnc.o_data))\n",
        "            #use regularizers for each layer of the controller\n",
        "            regularizers = (tf.nn.l2_loss(dnc.W1) + tf.nn.l2_loss(dnc.W2) +\n",
        "                            tf.nn.l2_loss(dnc.b1) + tf.nn.l2_loss(dnc.b2))\n",
        "            #to help the loss convergence faster\n",
        "            loss += 5e-4 * regularizers\n",
        "            #optimize the entire thing (memory + controller) using gradient descent. dope\n",
        "            optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
        "            \n",
        "            #initialize input output pairs\n",
        "            tf.initialize_all_variables().run()\n",
        "            final_i_data = np.concatenate((seq, zer), axis=0)\n",
        "            final_o_data = np.concatenate((zer, seq), axis=0)\n",
        "            #for each iteration\n",
        "            for i in range(0, iterations+1):\n",
        "                #feed in each input output pair\n",
        "                feed_dict = {dnc.i_data: final_i_data, dnc.o_data: final_o_data}\n",
        "                #make predictions\n",
        "                l, _, predictions = sess.run([loss, optimizer, output], feed_dict=feed_dict)\n",
        "                if i%100==0:\n",
        "                    print(i,l)\n",
        "            #print results\n",
        "            print(final_i_data)\n",
        "            print(final_o_data)\n",
        "            print(predictions)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    tf.app.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "m6xBvM3kUiyk",
        "outputId": "9f2d5574-901e-44ae-a876-156bebe2c15e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W1001 07:00:42.458451 140563578615680 deprecation.py:343] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/tf_should_use.py:243: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.6957412\n",
            "100 0.27570504\n",
            "200 0.17398405\n",
            "300 0.13471794\n",
            "400 0.11024215\n",
            "500 0.09543524\n",
            "600 0.06494595\n",
            "700 0.044430062\n",
            "800 0.0338293\n",
            "900 0.027010752\n",
            "1000 0.02241783\n",
            "[[0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 1. 0. 0.]]\n",
            "[[ -8.356178   -4.076645   -4.3553123  -5.6967163]\n",
            " [ -6.7735205  -6.643283   -6.5407186  -8.578088 ]\n",
            " [ -9.871399   -8.0376005  -4.1238728  -9.661024 ]\n",
            " [ -4.8676953  -9.685485   -9.942361  -13.019571 ]\n",
            " [ -8.9361     -9.401272   -3.1975641 -10.98782  ]\n",
            " [ -2.7232757 -10.565248  -11.182824  -15.514982 ]\n",
            " [ -6.012829   -9.328317    3.0625536  -9.522257 ]\n",
            " [  1.6560154  -5.5685735  -6.0249376 -10.414843 ]\n",
            " [ -2.7918103  -8.370953    3.1989026  -9.515444 ]\n",
            " [  5.3935657  -4.1628027  -4.62315   -12.224441 ]\n",
            " [ -3.6559436  -8.140218    9.634943   -3.122532 ]\n",
            " [ -2.4803743   2.7334824  -8.915469   -3.336048 ]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    }
  ]
}