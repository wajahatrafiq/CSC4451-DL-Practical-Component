{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "lGuophYWX076"
      },
      "source": [
        "# Language Translation\n",
        "\n",
        "## Get the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r74hROTdX08C"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "def load_data(path):\n",
        "    input_file = os.path.join(path)\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        data = f.read()\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuhc37gWX08F"
      },
      "outputs": [],
      "source": [
        "source_path = 'data/small_vocab_en'\n",
        "target_path = 'data/small_vocab_fr'\n",
        "source_text = load_data(source_path)\n",
        "target_text = load_data(target_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dslrq55uX08G"
      },
      "source": [
        "## Explore the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTW29VPGX08H",
        "outputId": "a654c6f4-2b39-4c83-82d7-93a4a8e5516f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset Brief Stats\n",
            "* number of unique words in English sample sentences: 227        [this is roughly measured/without any preprocessing]\n",
            "\n",
            "* English sentences\n",
            "\t- number of sentences: 137861\n",
            "\t- avg. number of words in a sentence: 13.225277634719028\n",
            "* French sentences\n",
            "\t- number of sentences: 137861 [data integrity check / should have the same number]\n",
            "\t- avg. number of words in a sentence: 14.226612312401622\n",
            "\n",
            "* Sample sentences range from 0 to 5\n",
            "[1-th] sentence\n",
            "\tEN: new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
            "\tFR: new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
            "\n",
            "[2-th] sentence\n",
            "\tEN: the united states is usually chilly during july , and it is usually freezing in november .\n",
            "\tFR: les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
            "\n",
            "[3-th] sentence\n",
            "\tEN: california is usually quiet during march , and it is usually hot in june .\n",
            "\tFR: california est généralement calme en mars , et il est généralement chaud en juin .\n",
            "\n",
            "[4-th] sentence\n",
            "\tEN: the united states is sometimes mild during june , and it is cold in september .\n",
            "\tFR: les états-unis est parfois légère en juin , et il fait froid en septembre .\n",
            "\n",
            "[5-th] sentence\n",
            "\tEN: your least liked fruit is the grape , but my least liked is the apple .\n",
            "\tFR: votre moins aimé fruit est le raisin , mais mon moins aimé est la pomme .\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "print('Dataset Brief Stats')\n",
        "print('* number of unique words in English sample sentences: {}\\\n",
        "        [this is roughly measured/without any preprocessing]'.format(len(Counter(source_text.split()))))\n",
        "print()\n",
        "\n",
        "english_sentences = source_text.split('\\n')\n",
        "print('* English sentences')\n",
        "print('\\t- number of sentences: {}'.format(len(english_sentences)))\n",
        "print('\\t- avg. number of words in a sentence: {}'.format(np.average([len(sentence.split()) for sentence in english_sentences])))\n",
        "\n",
        "french_sentences = target_text.split('\\n')\n",
        "print('* French sentences')\n",
        "print('\\t- number of sentences: {} [data integrity check / should have the same number]'.format(len(french_sentences)))\n",
        "print('\\t- avg. number of words in a sentence: {}'.format(np.average([len(sentence.split()) for sentence in french_sentences])))\n",
        "print()\n",
        "\n",
        "sample_sentence_range = (0, 5)\n",
        "side_by_side_sentences = list(zip(english_sentences, french_sentences))[sample_sentence_range[0]:sample_sentence_range[1]]\n",
        "print('* Sample sentences range from {} to {}'.format(sample_sentence_range[0], sample_sentence_range[1]))\n",
        "\n",
        "for index, sentence in enumerate(side_by_side_sentences):\n",
        "    en_sent, fr_sent = sentence\n",
        "    print('[{}-th] sentence'.format(index+1))\n",
        "    print('\\tEN: {}'.format(en_sent))\n",
        "    print('\\tFR: {}'.format(fr_sent))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaR0V1stX08M"
      },
      "source": [
        "## Preprocessing\n",
        "\n",
        "### Create Lookup Tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rdOk1RGX08P"
      },
      "outputs": [],
      "source": [
        "CODES = {'<PAD>': 0, '<EOS>': 1, '<UNK>': 2, '<GO>': 3 }\n",
        "\n",
        "def create_lookup_tables(text):\n",
        "    # make a list of unique words\n",
        "    vocab = set(text.split())\n",
        "\n",
        "    # (1)\n",
        "    # starts with the special tokens\n",
        "    vocab_to_int = copy.copy(CODES)\n",
        "\n",
        "    # the index (v_i) will starts from 4 (the 2nd arg in enumerate() specifies the starting index)\n",
        "    # since vocab_to_int already contains special tokens\n",
        "    for v_i, v in enumerate(vocab, len(CODES)):\n",
        "        vocab_to_int[v] = v_i\n",
        "\n",
        "    # (2)\n",
        "    int_to_vocab = {v_i: v for v, v_i in vocab_to_int.items()}\n",
        "\n",
        "    return vocab_to_int, int_to_vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dsGnBv-X08R"
      },
      "source": [
        "### Text to Word Ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlEycxHGX08T"
      },
      "outputs": [],
      "source": [
        "def text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int):\n",
        "    \"\"\"\n",
        "        1st, 2nd args: raw string text to be converted\n",
        "        3rd, 4th args: lookup tables for 1st and 2nd args respectively\n",
        "    \n",
        "        return: A tuple of lists (source_id_text, target_id_text) converted\n",
        "    \"\"\"\n",
        "    # empty list of converted sentences\n",
        "    source_text_id = []\n",
        "    target_text_id = []\n",
        "    \n",
        "    # make a list of sentences (extraction)\n",
        "    source_sentences = source_text.split(\"\\n\")\n",
        "    target_sentences = target_text.split(\"\\n\")\n",
        "    \n",
        "    max_source_sentence_length = max([len(sentence.split(\" \")) for sentence in source_sentences])\n",
        "    max_target_sentence_length = max([len(sentence.split(\" \")) for sentence in target_sentences])\n",
        "    \n",
        "    # iterating through each sentences (# of sentences in source&target is the same)\n",
        "    for i in range(len(source_sentences)):\n",
        "        # extract sentences one by one\n",
        "        source_sentence = source_sentences[i]\n",
        "        target_sentence = target_sentences[i]\n",
        "        \n",
        "        # make a list of tokens/words (extraction) from the chosen sentence\n",
        "        source_tokens = source_sentence.split(\" \")\n",
        "        target_tokens = target_sentence.split(\" \")\n",
        "        \n",
        "        # empty list of converted words to index in the chosen sentence\n",
        "        source_token_id = []\n",
        "        target_token_id = []\n",
        "        \n",
        "        for index, token in enumerate(source_tokens):\n",
        "            if (token != \"\"):\n",
        "                source_token_id.append(source_vocab_to_int[token])\n",
        "        \n",
        "        for index, token in enumerate(target_tokens):\n",
        "            if (token != \"\"):\n",
        "                target_token_id.append(target_vocab_to_int[token])\n",
        "                \n",
        "        # put <EOS> token at the end of the chosen target sentence\n",
        "        # this token suggests when to stop creating a sequence\n",
        "        target_token_id.append(target_vocab_to_int['<EOS>'])\n",
        "            \n",
        "        # add each converted sentences in the final list\n",
        "        source_text_id.append(source_token_id)\n",
        "        target_text_id.append(target_token_id)\n",
        "    \n",
        "    return source_text_id, target_text_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmSstLQJX08U"
      },
      "source": [
        "### Preprocess and Save Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUCLRWGyX08V"
      },
      "outputs": [],
      "source": [
        "def preprocess_and_save_data(source_path, target_path, text_to_ids):\n",
        "    # Preprocess\n",
        "    \n",
        "    # load original data (English, French)\n",
        "    source_text = load_data(source_path)\n",
        "    target_text = load_data(target_path)\n",
        "\n",
        "    # to the lower case\n",
        "    source_text = source_text.lower()\n",
        "    target_text = target_text.lower()\n",
        "\n",
        "    # create lookup tables for English and French data\n",
        "    source_vocab_to_int, source_int_to_vocab = create_lookup_tables(source_text)\n",
        "    target_vocab_to_int, target_int_to_vocab = create_lookup_tables(target_text)\n",
        "\n",
        "    # create list of sentences whose words are represented in index\n",
        "    source_text, target_text = text_to_ids(source_text, target_text, source_vocab_to_int, target_vocab_to_int)\n",
        "\n",
        "    # Save data for later use\n",
        "    pickle.dump((\n",
        "        (source_text, target_text),\n",
        "        (source_vocab_to_int, target_vocab_to_int),\n",
        "        (source_int_to_vocab, target_int_to_vocab)), open('preprocess.p', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fX402hWX08X"
      },
      "outputs": [],
      "source": [
        "preprocess_and_save_data(source_path, target_path, text_to_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaU983B7X08Y"
      },
      "source": [
        "# Check Point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjhfBtFEX08Y"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "def load_preprocess():\n",
        "    with open('preprocess.p', mode='rb') as in_file:\n",
        "        return pickle.load(in_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTRJ-HBsX08a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = load_preprocess()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJo0xP6TX08b"
      },
      "source": [
        "### Check the Version of TensorFlow and Access to GPU\n",
        "Since the Recurrent Neural Networks is kind of heavy model to train, it is recommended to train the model in GPU environment. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGrEBfYeX08c",
        "outputId": "92d1d49f-e6ab-4007-c7d7-159a84c67c79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow Version: 1.7.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/test/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: No GPU found. Please use a GPU to train your neural network.\n",
            "  if sys.path[0] == '':\n"
          ]
        }
      ],
      "source": [
        "from distutils.version import LooseVersion\n",
        "import warnings\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.layers.core import Dense\n",
        "\n",
        "# Check TensorFlow Version\n",
        "assert LooseVersion(tf.__version__) >= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'\n",
        "print('TensorFlow Version: {}'.format(tf.__version__))\n",
        "\n",
        "# Check for a GPU\n",
        "if not tf.test.gpu_device_name():\n",
        "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
        "else:\n",
        "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5ndFVJKX08d"
      },
      "source": [
        "## Build the Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxZsK3vjX08e"
      },
      "source": [
        "### Input (1), (3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYrRkXCFX08f"
      },
      "outputs": [],
      "source": [
        "def enc_dec_model_inputs():\n",
        "    inputs = tf.placeholder(tf.int32, [None, None], name='input')\n",
        "    targets = tf.placeholder(tf.int32, [None, None], name='targets') \n",
        "    \n",
        "    target_sequence_length = tf.placeholder(tf.int32, [None], name='target_sequence_length')\n",
        "    max_target_len = tf.reduce_max(target_sequence_length)    \n",
        "    \n",
        "    return inputs, targets, target_sequence_length, max_target_len"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d2awwMbX08g"
      },
      "source": [
        "`hyperparam_inputs` function creates and returns parameters (TF placeholders) related to hyper-parameters to the model. \n",
        "- lr_rate is learning rate\n",
        "- keep_prob is the keep probability for Dropouts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8h8DrbV1X08g"
      },
      "outputs": [],
      "source": [
        "def hyperparam_inputs():\n",
        "    lr_rate = tf.placeholder(tf.float32, name='lr_rate')\n",
        "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "    \n",
        "    return lr_rate, keep_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPPgdI1bX08h"
      },
      "source": [
        "### Process Decoder Input (3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1M71aRk-X08i"
      },
      "outputs": [],
      "source": [
        "def process_decoder_input(target_data, target_vocab_to_int, batch_size):\n",
        "    \"\"\"\n",
        "    Preprocess target data for encoding\n",
        "    :return: Preprocessed target data\n",
        "    \"\"\"\n",
        "    # get '<GO>' id\n",
        "    go_id = target_vocab_to_int['<GO>']\n",
        "    \n",
        "    after_slice = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
        "    after_concat = tf.concat( [tf.fill([batch_size, 1], go_id), after_slice], 1)\n",
        "    \n",
        "    return after_concat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u72-9RoxX08j"
      },
      "source": [
        "### Encoding (2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "z-CxW707X08k"
      },
      "outputs": [],
      "source": [
        "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, \n",
        "                   source_vocab_size, \n",
        "                   encoding_embedding_size):\n",
        "    \"\"\"\n",
        "    :return: tuple (RNN output, RNN state)\n",
        "    \"\"\"\n",
        "    embed = tf.contrib.layers.embed_sequence(rnn_inputs, \n",
        "                                             vocab_size=source_vocab_size, \n",
        "                                             embed_dim=encoding_embedding_size)\n",
        "    \n",
        "    stacked_cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.LSTMCell(rnn_size), keep_prob) for _ in range(num_layers)])\n",
        "    \n",
        "    outputs, state = tf.nn.dynamic_rnn(stacked_cells, \n",
        "                                       embed, \n",
        "                                       dtype=tf.float32)\n",
        "    return outputs, state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MwB8EgwX08l"
      },
      "source": [
        "### Decoding - Training process (4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjPlFsoUX08m"
      },
      "outputs": [],
      "source": [
        "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, \n",
        "                         target_sequence_length, max_summary_length, \n",
        "                         output_layer, keep_prob):\n",
        "    \"\"\"\n",
        "    Create a training process in decoding layer \n",
        "    :return: BasicDecoderOutput containing training logits and sample_id\n",
        "    \"\"\"\n",
        "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, \n",
        "                                             output_keep_prob=keep_prob)\n",
        "    \n",
        "    # for only input layer\n",
        "    helper = tf.contrib.seq2seq.TrainingHelper(dec_embed_input, \n",
        "                                               target_sequence_length)\n",
        "    \n",
        "    decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n",
        "                                              helper, \n",
        "                                              encoder_state, \n",
        "                                              output_layer)\n",
        "\n",
        "    # unrolling the decoder layer\n",
        "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n",
        "                                                      impute_finished=True, \n",
        "                                                      maximum_iterations=max_summary_length)\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaYw0FsdX08m"
      },
      "source": [
        "### Decoding - Inference process (5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "o5o-AIb9X08n"
      },
      "outputs": [],
      "source": [
        "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id,\n",
        "                         end_of_sequence_id, max_target_sequence_length,\n",
        "                         vocab_size, output_layer, batch_size, keep_prob):\n",
        "    \"\"\"\n",
        "    Create a inference process in decoding layer \n",
        "    :return: BasicDecoderOutput containing inference logits and sample_id\n",
        "    \"\"\"\n",
        "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, \n",
        "                                             output_keep_prob=keep_prob)\n",
        "    \n",
        "    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings, \n",
        "                                                      tf.fill([batch_size], start_of_sequence_id), \n",
        "                                                      end_of_sequence_id)\n",
        "    \n",
        "    decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, \n",
        "                                              helper, \n",
        "                                              encoder_state, \n",
        "                                              output_layer)\n",
        "    \n",
        "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n",
        "                                                      impute_finished=True, \n",
        "                                                      maximum_iterations=max_target_sequence_length)\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEAGQf7CX08o"
      },
      "source": [
        "### Build the Decoding Layer (3), (6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-i-dtz6QX08p"
      },
      "outputs": [],
      "source": [
        "def decoding_layer(dec_input, encoder_state,\n",
        "                   target_sequence_length, max_target_sequence_length,\n",
        "                   rnn_size,\n",
        "                   num_layers, target_vocab_to_int, target_vocab_size,\n",
        "                   batch_size, keep_prob, decoding_embedding_size):\n",
        "    \"\"\"\n",
        "    Create decoding layer\n",
        "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
        "    \"\"\"\n",
        "    target_vocab_size = len(target_vocab_to_int)\n",
        "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
        "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
        "    \n",
        "    cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(rnn_size) for _ in range(num_layers)])\n",
        "    \n",
        "    with tf.variable_scope(\"decode\"):\n",
        "        output_layer = tf.layers.Dense(target_vocab_size)\n",
        "        train_output = decoding_layer_train(encoder_state, \n",
        "                                            cells, \n",
        "                                            dec_embed_input, \n",
        "                                            target_sequence_length, \n",
        "                                            max_target_sequence_length, \n",
        "                                            output_layer, \n",
        "                                            keep_prob)\n",
        "\n",
        "    with tf.variable_scope(\"decode\", reuse=True):\n",
        "        infer_output = decoding_layer_infer(encoder_state, \n",
        "                                            cells, \n",
        "                                            dec_embeddings, \n",
        "                                            target_vocab_to_int['<GO>'], \n",
        "                                            target_vocab_to_int['<EOS>'], \n",
        "                                            max_target_sequence_length, \n",
        "                                            target_vocab_size, \n",
        "                                            output_layer,\n",
        "                                            batch_size,\n",
        "                                            keep_prob)\n",
        "\n",
        "    return (train_output, infer_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cU_dN0YX08q"
      },
      "source": [
        "### Build the Seq2Seq model (7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZZCpDu4X08r"
      },
      "outputs": [],
      "source": [
        "def seq2seq_model(input_data, target_data, keep_prob, batch_size,\n",
        "                  target_sequence_length,\n",
        "                  max_target_sentence_length,\n",
        "                  source_vocab_size, target_vocab_size,\n",
        "                  enc_embedding_size, dec_embedding_size,\n",
        "                  rnn_size, num_layers, target_vocab_to_int):\n",
        "    \"\"\"\n",
        "    Build the Sequence-to-Sequence model\n",
        "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
        "    \"\"\"\n",
        "    enc_outputs, enc_states = encoding_layer(input_data, \n",
        "                                             rnn_size, \n",
        "                                             num_layers, \n",
        "                                             keep_prob, \n",
        "                                             source_vocab_size, \n",
        "                                             enc_embedding_size)\n",
        "    \n",
        "    dec_input = process_decoder_input(target_data, \n",
        "                                      target_vocab_to_int, \n",
        "                                      batch_size)\n",
        "    \n",
        "    train_output, infer_output = decoding_layer(dec_input,\n",
        "                                               enc_states, \n",
        "                                               target_sequence_length, \n",
        "                                               max_target_sentence_length,\n",
        "                                               rnn_size,\n",
        "                                              num_layers,\n",
        "                                              target_vocab_to_int,\n",
        "                                              target_vocab_size,\n",
        "                                              batch_size,\n",
        "                                              keep_prob,\n",
        "                                              dec_embedding_size)\n",
        "    \n",
        "    return train_output, infer_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLaJg4MeX08s"
      },
      "source": [
        "## Neural Network Training\n",
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQNSlUo9X08s"
      },
      "outputs": [],
      "source": [
        "display_step = 300\n",
        "\n",
        "epochs = 13\n",
        "batch_size = 128\n",
        "\n",
        "rnn_size = 128\n",
        "num_layers = 3\n",
        "\n",
        "encoding_embedding_size = 200\n",
        "decoding_embedding_size = 200\n",
        "\n",
        "learning_rate = 0.001\n",
        "keep_probability = 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2Vi3YDQX08t"
      },
      "source": [
        "### Build the Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRlvP516X08u",
        "outputId": "5dd4b645-a13d-4ff5-95db-2f23ac9caf5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /anaconda/envs/test/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use the retry module or similar alternatives.\n"
          ]
        }
      ],
      "source": [
        "save_path = 'checkpoints/dev'\n",
        "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = load_preprocess()\n",
        "max_target_sentence_length = max([len(sentence) for sentence in source_int_text])\n",
        "\n",
        "train_graph = tf.Graph()\n",
        "with train_graph.as_default():\n",
        "    input_data, targets, target_sequence_length, max_target_sequence_length = enc_dec_model_inputs()\n",
        "    lr, keep_prob = hyperparam_inputs()\n",
        "    \n",
        "    train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
        "                                                   targets,\n",
        "                                                   keep_prob,\n",
        "                                                   batch_size,\n",
        "                                                   target_sequence_length,\n",
        "                                                   max_target_sequence_length,\n",
        "                                                   len(source_vocab_to_int),\n",
        "                                                   len(target_vocab_to_int),\n",
        "                                                   encoding_embedding_size,\n",
        "                                                   decoding_embedding_size,\n",
        "                                                   rnn_size,\n",
        "                                                   num_layers,\n",
        "                                                   target_vocab_to_int)\n",
        "    \n",
        "    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n",
        "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
        "\n",
        "    # https://www.tensorflow.org/api_docs/python/tf/sequence_mask\n",
        "    # - Returns a mask tensor representing the first N positions of each cell.\n",
        "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
        "\n",
        "    with tf.name_scope(\"optimization\"):\n",
        "        # Loss function - weighted softmax cross entropy\n",
        "        cost = tf.contrib.seq2seq.sequence_loss(\n",
        "            training_logits,\n",
        "            targets,\n",
        "            masks)\n",
        "\n",
        "        # Optimizer\n",
        "        optimizer = tf.train.AdamOptimizer(lr)\n",
        "\n",
        "        # Gradient Clipping\n",
        "        gradients = optimizer.compute_gradients(cost)\n",
        "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
        "        train_op = optimizer.apply_gradients(capped_gradients)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4_vExRcX08w"
      },
      "source": [
        "### Get Batches and Pad the source and target sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DtbmGUVX08w"
      },
      "outputs": [],
      "source": [
        "def pad_sentence_batch(sentence_batch, pad_int):\n",
        "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
        "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
        "    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n",
        "\n",
        "\n",
        "def get_batches(sources, targets, batch_size, source_pad_int, target_pad_int):\n",
        "    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n",
        "    for batch_i in range(0, len(sources)//batch_size):\n",
        "        start_i = batch_i * batch_size\n",
        "\n",
        "        # Slice the right amount for the batch\n",
        "        sources_batch = sources[start_i:start_i + batch_size]\n",
        "        targets_batch = targets[start_i:start_i + batch_size]\n",
        "\n",
        "        # Pad\n",
        "        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n",
        "        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n",
        "\n",
        "        # Need the lengths for the _lengths parameters\n",
        "        pad_targets_lengths = []\n",
        "        for target in pad_targets_batch:\n",
        "            pad_targets_lengths.append(len(target))\n",
        "\n",
        "        pad_source_lengths = []\n",
        "        for source in pad_sources_batch:\n",
        "            pad_source_lengths.append(len(source))\n",
        "\n",
        "        yield pad_sources_batch, pad_targets_batch, pad_source_lengths, pad_targets_lengths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwN72nrHX08x"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "GCkq2bPCX08y",
        "outputId": "c5f56f64-3358-43ce-aba3-c27bcc8ace97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   0 Batch  300/1077 - Train Accuracy: 0.4293, Validation Accuracy: 0.5064, Loss: 1.9365\n",
            "Epoch   0 Batch  600/1077 - Train Accuracy: 0.4974, Validation Accuracy: 0.5241, Loss: 1.1117\n",
            "Epoch   0 Batch  900/1077 - Train Accuracy: 0.5410, Validation Accuracy: 0.5614, Loss: 0.9078\n",
            "Epoch   1 Batch  300/1077 - Train Accuracy: 0.5958, Validation Accuracy: 0.6044, Loss: 0.6807\n",
            "Epoch   1 Batch  600/1077 - Train Accuracy: 0.6786, Validation Accuracy: 0.6346, Loss: 0.5282\n",
            "Epoch   1 Batch  900/1077 - Train Accuracy: 0.7066, Validation Accuracy: 0.6779, Loss: 0.5059\n",
            "Epoch   2 Batch  300/1077 - Train Accuracy: 0.7512, Validation Accuracy: 0.7383, Loss: 0.4164\n",
            "Epoch   2 Batch  600/1077 - Train Accuracy: 0.7894, Validation Accuracy: 0.7784, Loss: 0.3555\n",
            "Epoch   2 Batch  900/1077 - Train Accuracy: 0.8164, Validation Accuracy: 0.7869, Loss: 0.3256\n",
            "Epoch   3 Batch  300/1077 - Train Accuracy: 0.8651, Validation Accuracy: 0.8224, Loss: 0.2444\n",
            "Epoch   3 Batch  600/1077 - Train Accuracy: 0.8519, Validation Accuracy: 0.8171, Loss: 0.2170\n",
            "Epoch   3 Batch  900/1077 - Train Accuracy: 0.8676, Validation Accuracy: 0.8509, Loss: 0.2241\n",
            "Epoch   4 Batch  300/1077 - Train Accuracy: 0.9219, Validation Accuracy: 0.8750, Loss: 0.1508\n",
            "Epoch   4 Batch  600/1077 - Train Accuracy: 0.9022, Validation Accuracy: 0.8984, Loss: 0.1493\n",
            "Epoch   4 Batch  900/1077 - Train Accuracy: 0.9105, Validation Accuracy: 0.8803, Loss: 0.1448\n",
            "Epoch   5 Batch  300/1077 - Train Accuracy: 0.9359, Validation Accuracy: 0.9091, Loss: 0.1008\n",
            "Epoch   5 Batch  600/1077 - Train Accuracy: 0.9364, Validation Accuracy: 0.9130, Loss: 0.1137\n",
            "Epoch   5 Batch  900/1077 - Train Accuracy: 0.9449, Validation Accuracy: 0.9205, Loss: 0.1105\n",
            "Epoch   6 Batch  300/1077 - Train Accuracy: 0.9548, Validation Accuracy: 0.9375, Loss: 0.0767\n",
            "Epoch   6 Batch  600/1077 - Train Accuracy: 0.9472, Validation Accuracy: 0.9347, Loss: 0.0794\n",
            "Epoch   6 Batch  900/1077 - Train Accuracy: 0.9441, Validation Accuracy: 0.9446, Loss: 0.0855\n",
            "Epoch   7 Batch  300/1077 - Train Accuracy: 0.9692, Validation Accuracy: 0.9364, Loss: 0.0530\n",
            "Epoch   7 Batch  600/1077 - Train Accuracy: 0.9483, Validation Accuracy: 0.9322, Loss: 0.0719\n",
            "Epoch   7 Batch  900/1077 - Train Accuracy: 0.9535, Validation Accuracy: 0.9513, Loss: 0.0616\n",
            "Epoch   8 Batch  300/1077 - Train Accuracy: 0.9667, Validation Accuracy: 0.9542, Loss: 0.0461\n",
            "Epoch   8 Batch  600/1077 - Train Accuracy: 0.9528, Validation Accuracy: 0.9506, Loss: 0.0595\n",
            "Epoch   8 Batch  900/1077 - Train Accuracy: 0.9656, Validation Accuracy: 0.9489, Loss: 0.0506\n",
            "Epoch   9 Batch  300/1077 - Train Accuracy: 0.9720, Validation Accuracy: 0.9563, Loss: 0.0383\n",
            "Epoch   9 Batch  600/1077 - Train Accuracy: 0.9624, Validation Accuracy: 0.9645, Loss: 0.0488\n",
            "Epoch   9 Batch  900/1077 - Train Accuracy: 0.9727, Validation Accuracy: 0.9638, Loss: 0.0511\n",
            "Epoch  10 Batch  300/1077 - Train Accuracy: 0.9753, Validation Accuracy: 0.9631, Loss: 0.0343\n",
            "Epoch  10 Batch  600/1077 - Train Accuracy: 0.9494, Validation Accuracy: 0.9748, Loss: 0.0482\n",
            "Epoch  10 Batch  900/1077 - Train Accuracy: 0.9746, Validation Accuracy: 0.9688, Loss: 0.0416\n",
            "Epoch  11 Batch  300/1077 - Train Accuracy: 0.9778, Validation Accuracy: 0.9737, Loss: 0.0258\n",
            "Epoch  11 Batch  600/1077 - Train Accuracy: 0.9513, Validation Accuracy: 0.9680, Loss: 0.0353\n",
            "Epoch  11 Batch  900/1077 - Train Accuracy: 0.9602, Validation Accuracy: 0.9712, Loss: 0.0419\n",
            "Epoch  12 Batch  300/1077 - Train Accuracy: 0.9774, Validation Accuracy: 0.9656, Loss: 0.0252\n",
            "Epoch  12 Batch  600/1077 - Train Accuracy: 0.9710, Validation Accuracy: 0.9698, Loss: 0.0413\n",
            "Epoch  12 Batch  900/1077 - Train Accuracy: 0.9770, Validation Accuracy: 0.9684, Loss: 0.0352\n",
            "Model Trained and Saved\n"
          ]
        }
      ],
      "source": [
        "def get_accuracy(target, logits):\n",
        "    \"\"\"\n",
        "    Calculate accuracy\n",
        "    \"\"\"\n",
        "    max_seq = max(target.shape[1], logits.shape[1])\n",
        "    if max_seq - target.shape[1]:\n",
        "        target = np.pad(\n",
        "            target,\n",
        "            [(0,0),(0,max_seq - target.shape[1])],\n",
        "            'constant')\n",
        "    if max_seq - logits.shape[1]:\n",
        "        logits = np.pad(\n",
        "            logits,\n",
        "            [(0,0),(0,max_seq - logits.shape[1])],\n",
        "            'constant')\n",
        "\n",
        "    return np.mean(np.equal(target, logits))\n",
        "\n",
        "# Split data to training and validation sets\n",
        "train_source = source_int_text[batch_size:]\n",
        "train_target = target_int_text[batch_size:]\n",
        "valid_source = source_int_text[:batch_size]\n",
        "valid_target = target_int_text[:batch_size]\n",
        "(valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths ) = next(get_batches(valid_source,\n",
        "                                                                                                             valid_target,\n",
        "                                                                                                             batch_size,\n",
        "                                                                                                             source_vocab_to_int['<PAD>'],\n",
        "                                                                                                             target_vocab_to_int['<PAD>']))                                                                                                  \n",
        "with tf.Session(graph=train_graph) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for epoch_i in range(epochs):\n",
        "        for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n",
        "                get_batches(train_source, train_target, batch_size,\n",
        "                            source_vocab_to_int['<PAD>'],\n",
        "                            target_vocab_to_int['<PAD>'])):\n",
        "\n",
        "            _, loss = sess.run(\n",
        "                [train_op, cost],\n",
        "                {input_data: source_batch,\n",
        "                 targets: target_batch,\n",
        "                 lr: learning_rate,\n",
        "                 target_sequence_length: targets_lengths,\n",
        "                 keep_prob: keep_probability})\n",
        "\n",
        "\n",
        "            if batch_i % display_step == 0 and batch_i > 0:\n",
        "                batch_train_logits = sess.run(\n",
        "                    inference_logits,\n",
        "                    {input_data: source_batch,\n",
        "                     target_sequence_length: targets_lengths,\n",
        "                     keep_prob: 1.0})\n",
        "\n",
        "                batch_valid_logits = sess.run(\n",
        "                    inference_logits,\n",
        "                    {input_data: valid_sources_batch,\n",
        "                     target_sequence_length: valid_targets_lengths,\n",
        "                     keep_prob: 1.0})\n",
        "\n",
        "                train_acc = get_accuracy(target_batch, batch_train_logits)\n",
        "                valid_acc = get_accuracy(valid_targets_batch, batch_valid_logits)\n",
        "\n",
        "                print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.4f}, Validation Accuracy: {:>6.4f}, Loss: {:>6.4f}'\n",
        "                      .format(epoch_i, batch_i, len(source_int_text) // batch_size, train_acc, valid_acc, loss))\n",
        "\n",
        "    # Save Model\n",
        "    saver = tf.train.Saver()\n",
        "    saver.save(sess, save_path)\n",
        "    print('Model Trained and Saved')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu0TK7tuX08z"
      },
      "source": [
        "### Save Parameters\n",
        "Save the `batch_size` and `save_path` parameters for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NB9PlnYrX080"
      },
      "outputs": [],
      "source": [
        "def save_params(params):\n",
        "    with open('params.p', 'wb') as out_file:\n",
        "        pickle.dump(params, out_file)\n",
        "\n",
        "\n",
        "def load_params():\n",
        "    with open('params.p', mode='rb') as in_file:\n",
        "        return pickle.load(in_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bLUfhDzX081"
      },
      "outputs": [],
      "source": [
        "# Save parameters for checkpoint\n",
        "save_params(save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzFW7wVsX081"
      },
      "source": [
        "# Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tXTvwYCX082"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import problem_unittests as tests\n",
        "\n",
        "_, (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = load_preprocess()\n",
        "load_path = load_params()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdgICilxX082"
      },
      "source": [
        "## Translate\n",
        "This will translate `translate_sentence` from English to French."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9r3dQsZ4X083",
        "outputId": "64de1362-5412-4235-da70-e196cf32eb8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from checkpoints/dev\n",
            "Input\n",
            "  Word Ids:      [158, 189, 82, 152, 206, 176, 101]\n",
            "  English Words: ['he', 'saw', 'a', 'old', 'yellow', 'truck', '.']\n",
            "\n",
            "Prediction\n",
            "  Word Ids:      [18, 212, 182, 26, 220, 48, 317, 126, 1]\n",
            "  French Words: il a vu un nouveau camion jaune . <EOS>\n"
          ]
        }
      ],
      "source": [
        "def sentence_to_seq(sentence, vocab_to_int):\n",
        "    results = []\n",
        "    for word in sentence.split(\" \"):\n",
        "        if word in vocab_to_int:\n",
        "            results.append(vocab_to_int[word])\n",
        "        else:\n",
        "            results.append(vocab_to_int['<UNK>'])\n",
        "            \n",
        "    return results\n",
        "\n",
        "translate_sentence = 'he saw a old yellow truck .'\n",
        "\n",
        "translate_sentence = sentence_to_seq(translate_sentence, source_vocab_to_int)\n",
        "\n",
        "loaded_graph = tf.Graph()\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
        "    loader.restore(sess, load_path)\n",
        "\n",
        "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
        "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
        "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
        "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "\n",
        "    translate_logits = sess.run(logits, {input_data: [translate_sentence]*batch_size,\n",
        "                                         target_sequence_length: [len(translate_sentence)*2]*batch_size,\n",
        "                                         keep_prob: 1.0})[0]\n",
        "\n",
        "print('Input')\n",
        "print('  Word Ids:      {}'.format([i for i in translate_sentence]))\n",
        "print('  English Words: {}'.format([source_int_to_vocab[i] for i in translate_sentence]))\n",
        "\n",
        "print('\\nPrediction')\n",
        "print('  Word Ids:      {}'.format([i for i in translate_logits]))\n",
        "print('  French Words: {}'.format(\" \".join([target_int_to_vocab[i] for i in translate_logits])))"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "test",
      "language": "python",
      "name": "test"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}